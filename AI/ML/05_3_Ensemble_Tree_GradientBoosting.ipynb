{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNulhOQPARNBGcvJFmvij66",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lee1201zxc/study/blob/main/AI/ML/05_3_Ensemble_Tree_GradientBoosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJj5IHWIVfgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 앙상블\n",
        "\n",
        "정형 데이터와 비정형 데이터(표현, 학습 어렵 -> 딥러닝으로)가 존재\n",
        "\n",
        "여러 모델을 결합해서 더 강한 모델 만듬\n",
        "###배깅\n",
        "같은 모델 여러 개를 독립적으로 학습\n",
        "###부스팅\n",
        "이전 모델의 실수를 다음 모델이 보완\n",
        "\n",
        "| 모델                   | 앙상블 방식 |\n",
        "| -------------------- | ------ |\n",
        "| RandomForest         | 배깅     |\n",
        "| ExtraTrees           | 배깅(강화) |\n",
        "| GradientBoosting     | 부스팅    |\n",
        "| HistGradientBoosting | 부스팅    |\n",
        "| XGBoost / LGBM       | 부스팅    |\n"
      ],
      "metadata": {
        "id": "sFr1eTR7mahZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kEAlHpNXjm5m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 랜덤 포레스트\n",
        "\n",
        "랜덤으로 샘플, 특성을 선택해서 과적합 방지\n",
        "기본적으로 100개의 결정 트리 훈련\n",
        "\n",
        "결정 트리를 랜덤하게 만들어 숲을 만들고 각각 예측해 최종 예측을 만듬\n",
        "\n",
        "샘플도, 특성도 랜덤 선택(전체 특성에 루트한 값)\n",
        "\n",
        "오버피팅 방지에 좋음\n",
        "<br><br>\n",
        "부트스트랩 샘플\n",
        "->데이터에서 중복을 허용하여 데이터를 샘플링"
      ],
      "metadata": {
        "id": "sQwdCpOw2oR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "data = wine[['alcohol', 'sugar', 'pH']]\n",
        "target = wine['class']\n",
        "train_input, test_input, train_target, test_target= train_test_split(data,target, test_size=0.2,random_state=42)\n",
        "\n",
        "#랜덤 포레스트\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42, oob_score=True)\n",
        "\n",
        "#교차 검증 진행\n",
        "scores= cross_validate(rt, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#과적합됨\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "print(\"결정트리보다 당도 중요도가 감소하고 나머지가 상승 -> 하나의 특성에 과도하게 집중되지 않도록 함\")\n",
        "print(rf.feature_importances_)\n",
        "# 부트스트랩 샘플 고른거 제외 나머지로 평가도 진행함\n",
        "print(rf.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrXFFJPawh7P",
        "outputId": "7c4764a9-a894-4cc1-cb6b-951caa4d89ca"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n",
            "결정트리보다 당도 중요도가 감소하고 나머지가 상승 -> 하나의 특성에 과도하게 집중되지 않도록 함\n",
            "[0.23167441 0.50039841 0.26792718]\n",
            "0.8934000384837406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##엑스트라 트리\n",
        "랜덤 포레스트와 차이점은 부트스트랩 샘플을 사용하지 않음. 일부 샘플이 아닌 전체 샘플로 훈련 진행,\n",
        "\n",
        "하지만 노드 분할시 가장 좋은 분할이 아닌 랜덤 분할함(splitter=random), 성능은 떨어지나 오버피팅을 막고 테스트 점수를 높임,분할에 시간 덜 써서 속도 빠름"
      ],
      "metadata": {
        "id": "V2ZBcEr-7UHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "et= ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "scores =cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_BB4vxz7WTC",
        "outputId": "24f2ee91-b66d-44a9-c413-1f2214ba8b8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n",
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##그레디언트 부스팅\n",
        "깊이가 얕은 결정 트리를 사용하여 이전 트리의 오차 보완\n",
        "\n",
        "기본적으로 깊이가 3인 트리 100개 사용, 경사 하강법 이용\n",
        "\n",
        "오버피팅에 강함\n",
        "\n",
        "랜덤 포레스트보다 좋은 성능 but 훈련 속도 느림(병렬 훈련 불가)"
      ],
      "metadata": {
        "id": "soyQ3gzJ_A5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb= GradientBoostingClassifier(random_state=42)\n",
        "scores= cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "# 결정 트리 개수 500개로 늘리고 학습률 0.2로늘림(기본 0.1)\n",
        "gb= GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores= cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pNy8E9X_Dm8",
        "outputId": "e75c21ed-163e-474d-b807-73b42219ca64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n",
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##히스토그램 기반 그래디언트 부스팅\n",
        "정형 데이터 다루는 머신러닝 알고리즘 중에 인기 가장 좋음\n",
        "입력 특성(훈련 데이터)을 256개 구간으로 나눔(히스토그램 구간).\n",
        "\n",
        "기존 Gradient Boosting에선 정확한 분할 후보를 일일이 탐색하나\n",
        "이건 연속값을 그대로 사용하지 않고 히스토그램 기반으로 값을 미리 구간으로 나누고 분기 탐색 후 트리 생성\n",
        "\n",
        "데이터가 크고 빠르게 구하고 싶을 때 사용, 일반 그레디언트보다 오버피팅 억제하면서 좋은 성능을 제공"
      ],
      "metadata": {
        "id": "B0B0uuQkDnHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores= cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "from sklearn.inspection import permutation_importance\n",
        "hgb.fit(train_input, train_target)\n",
        "# 안에 중요도(importances), 평균(importances_mean), 표준 편차(importances_std)\n",
        "# n_repeats -> 매개변수 랜덤하게 섞을 확률\n",
        "result= permutation_importance(hgb, train_input, train_target, n_repeats=10, random_state=42,n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "print()\n",
        "\n",
        "result= permutation_importance(hgb, test_input, test_target, n_repeats=10, random_state=42,n_jobs=-1)\n",
        "print(result.importances_mean)\n",
        "hgb.score(test_input, test_target)\n",
        "\n",
        "############################################################\n",
        "#히스토그램 기반 그레디언트 부스팅 알고리즘 라이브러리\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "scores= cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "\n",
        "#히스토그램 기반 그레디언트 부스팅 라이브러리(마이크로소프트)\n",
        "from lightgbm import LGBMClassifier\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "scores= cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=1)\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLu1UQ6JG2ch",
        "outputId": "feec156c-8b9f-4f65-fbe3-fc8c2d49f9f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n",
            "[0.08876275 0.23438522 0.08027708]\n",
            "\n",
            "[0.05969231 0.20238462 0.049     ]\n",
            "0.9567059184812372 0.8783915747390243\n",
            "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 372\n",
            "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738\n",
            "[LightGBM] [Info] Start training from score 1.141738\n",
            "[LightGBM] [Info] Number of positive: 3151, number of negative: 1006\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000287 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 370\n",
            "[LightGBM] [Info] Number of data points in the train set: 4157, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757999 -> initscore=1.141738\n",
            "[LightGBM] [Info] Start training from score 1.141738\n",
            "[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000261 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 373\n",
            "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744\n",
            "[LightGBM] [Info] Start training from score 1.140744\n",
            "[LightGBM] [Info] Number of positive: 3151, number of negative: 1007\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000178 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 372\n",
            "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.757816 -> initscore=1.140744\n",
            "[LightGBM] [Info] Start training from score 1.140744\n",
            "[LightGBM] [Info] Number of positive: 3152, number of negative: 1006\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 371\n",
            "[LightGBM] [Info] Number of data points in the train set: 4158, number of used features: 3\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.758057 -> initscore=1.142055\n",
            "[LightGBM] [Info] Start training from score 1.142055\n",
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ]
    }
  ]
}